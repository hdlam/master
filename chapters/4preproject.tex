\section{Traffic Sim, pre-project}
For my master's pre-project I worked on a traffic simulator.
The traffic simulator is a project for Statens Vegvesenet Teknologidagene in Trondheim where the ChIRP Robots were used to demonstrate the concept of platooning. 
The idea of platooning is to use an autonomous system (either global centralized system or each individual entity communicates with each other) to control the vehicles, for instance cars, trucks etc. as one unit thus increasing the flow of traffic. When the red traffic light changes to green light, drivers usually drive one by one. The driver only drives forward when there's enough space in front of his or her car, in other words, the drivers are trying to follow the 3 second rule.
However with platooning and autonomous steering/driving the cars will drive instantly when the red light changes to green, and it will look like all the cars drive as one unit, or like a train if you will.
This leads to less air resistance and the cars will be able to cross an intersection much faster.

The whole system used a web camera with OpenCV to track the robots, all the robots were equipped with a bluetooth module and 2 circular post-it notes which was light green and pink. The camera was able to detect these post-it notes and track the position and rotation of each robot. This information was then sent to the simulation written in Java via UDP packets.
The simulation then calculated where the robot needed to go, set the speed for each wheel and sent the commands to the robot using the bluetooth COM-port.

The java simulation was first implemented by Magnus Hu using a game framework called Lightweight Java Game Library (LWJGL) to render everything on screen. We decided to change the framework from LWJGL to Slick2D. Slick2D is a 2D Java game library which uses tools and utilities wrapped around LWJGL, which means that it can do everything LWJGL can do, but also has higher abstraction. Slick2D lets us render shapes (circles, squares) without having to specify each point and line of the shape manually which we had to do in LWJGL. We could also easily check if the shapes intersected each other using the built in method "intersects" for the Shape class.
The track used in the simulation consisted of 24 points (a point as in a point in space) laid out in a 8-shaped layout, where the center intersection is made up of 2 points. For every point in the list, a line would be drawn between a point and it 2 neighbors in the list, this formed the 8-shaped figure which was our track.
A prototype image of a 8 shaped track was used to calculate the points needed, and the same image was projected onto our physical robot 
Before running the OpenCV software and the Java simulation, both application needs to set a configuration parameter of how many robots are to be tracked/run. Our demo at teknologidagene used 4 robots because we only had 4 bluetooth modules. The bluetooth modules had to be connected and added as a COM port before running the simulation or else it wouldn't know where to send the commands. When starting both the OpenCV camera tracking software and the simulation, coordinates of each robot along with the rotation alignment were tracked and sent via UDP packets. At this point in time, the simulation doesn't know which coordinate and rotation belongs to which bluetooth COM port. To map the COM port to a robot the simulator does the following for each robot:
\begin{itemize}
    \item checks the angle of all the robots
    \item sends a rotate command to the bluetooth COM port, which makes one robot rotate approximately 90 degrees.
    \item checks the angle of all the robots, finds out which one has rotated the most, saves this COM port to the robot which have rotated the most
    \item rotates the robot back to the original orientation
\end{itemize}
The reason for rotating the robot back to the original orientation is because we always laid the robot out in the map facing forward, which was easier to implement instead of implementing a lot of code to ensure that the robot would face forward if it started out in the wrong direction.
Each robot will now have a COM port given that the COM port didn't time out on initiation, in which we had to restart the whole simulation program to reconnect to the COM port.
The simulation then calculates the robot's nearest point on the track, and sets the target point of the robot to be point after the nearest one, this is in case the nearest point is behind the robot. After the simulation found out where the robot should move to, it calculates which way the robot needed to rotate and which speed it should have. The speed and the amount of needed rotation ($\Delta angle$) is then converted to <"two motors"> which is sent as a byte array of 4 elements to the robot's bluetooth module. An example of the byte array could be "l40r50" which means that the right motor would drive a little bit faster than the left one thus turning the robot to the left.
If we used a capital 'L' or 'R' in the byte array instead, the wheel would turn backwards, however we don't use that in the simulation at all except for turning the robot in the beginning when it tries to assign the COM port to the correct robot.

The simulator creates a box in front of each robot that will be used to find out whether the robot has a robot in front of it or not. If there's a robot in front (that is inside the front box) then the simulator will check if platooning is on or not. If platooning mode is on, the robot will try to keep the same speed as the robot in front of it. At least it's not allowed to have higher speed than the robot in front of it, or else it will crash into it. If platooning mode is off, then the robot will try to keep a 3 second distance from the robot in front, that is the robot will try to keep the robot in front of it outside the detection box. The robot is considered as reaching a target if the robot's center is 20 pixels away from the target, and the next point will be assigned as the new target for the robot.
In the middle of the map, at the intersection there is a red light at the top, north side or at the right, east side of the intersection. This red light will be alternating between these two positions depending on what the user chooses. 
